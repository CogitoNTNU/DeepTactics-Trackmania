{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"![GitHub Workflow Status (with event)](https://img.shields.io/github/actions/workflow/status/CogitoNTNU/DeepTactics-TrackMania/ci.yml) ![GitHub top language](https://img.shields.io/github/languages/top/CogitoNTNU/DeepTactics-TrackMania) ![GitHub language count](https://img.shields.io/github/languages/count/CogitoNTNU/DeepTactics-TrackMania) [![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT) [![Project Version](https://img.shields.io/badge/version-0.0.1-blue)](https://img.shields.io/badge/version-0.0.1-blue)   \ud83d\udccb Table of contents   - [DeepTactics-TrackMania](#deeptactics-trackmania)   - [\ud83c\udfaf Project Goals](#-project-goals)   - [\ud83e\udde0 Project Description](#-project-description)   - [\ud83c\udfd7\ufe0f Architecture \\&amp; Tech Stack](#\ufe0f-architecture--tech-stack)   - [\ud83d\udcda Key Resources](#-key-resources)   - [\ud83d\ude80 Getting Started](#-getting-started)   - [Description](#description)   - [\ud83d\udee0\ufe0f Prerequisites](#\ufe0f-prerequisites)   - [Getting started](#getting-started)   - [Usage](#usage)     - [\ud83d\udcd6 Generate Documentation Site](#-generate-documentation-site)   - [Testing](#testing)   - [\ud83d\udc65 Team](#-team)     - [Project Leads](#project-leads)     - [License](#license)"},{"location":"#deeptactics-trackmania","title":"DeepTactics-TrackMania","text":"<p>\ud83d\ude97 Deeptactics Trackmania is a student-driven project exploring Reinforcement Learning (RL) in the racing game Trackmania. Our goal is to design, train, and visualize agents capable of completing tracks, improving over time, and eventually outperforming human players in our group.</p>"},{"location":"#project-goals","title":"\ud83c\udfaf Project Goals","text":"<ul> <li> <p>Main Goal:   Build an RL system that can successfully complete a Trackmania track.</p> </li> <li> <p>Subgoals: </p> </li> <li>Achieve podium placement within the group.  </li> <li>Beat all group members on at least one track.  </li> <li>Visualize the agent inside the game.  </li> <li>Ensure all members gain hands-on RL experience.  </li> <li>Understand exploration vs exploitation.  </li> <li>Enable everyone to start training runs on their own PC.  </li> <li>Promote collaboration: every line of code should be understood by the group.  </li> <li>Document progress with a short film showing the agent\u2019s improvement.  </li> </ul>"},{"location":"#project-description","title":"\ud83e\udde0 Project Description","text":"<p>We aim to train RL agents using a variety of methods (PPO, DQN variants, SAC, IQN, etc.) in Trackmania. The project emphasizes:  </p> <ul> <li>Experimenting with multiple RL approaches.  </li> <li>Building shared knowledge through research workshops.  </li> <li>Using visualization and metrics dashboards (e.g. WandB) to monitor progress.  </li> <li>Combining technical learning with social team-building.  </li> </ul>"},{"location":"#architecture-tech-stack","title":"\ud83c\udfd7\ufe0f Architecture &amp; Tech Stack","text":"<ul> <li>Environment: Gymnasium + TMRL / TMInterface </li> <li>RL Algorithms: PPO, SAC, DQN variants (including IQN)  </li> <li>Experiment Tracking: Weights &amp; Biases (WandB)  </li> <li>Tooling: Git, Docker, GitHub Actions, pre-commit  </li> <li>Visualization: Custom render scripts for agent playback  </li> </ul>"},{"location":"#key-resources","title":"\ud83d\udcda Key Resources","text":"<ul> <li>PPO Paper (Schulman et al.) </li> <li>SAC Paper (Haarnoja et al.) </li> <li>IQN Paper </li> <li>TMRL Framework </li> <li>Linesight RL (YouTube) </li> <li>TMUnlimiter </li> </ul>"},{"location":"#getting-started","title":"\ud83d\ude80 Getting Started","text":"<ol> <li>Clone repo &amp; install dependencies (Docker setup provided).  </li> <li>Configure environment (Gymnasium + Trackmania interface).  </li> <li>Run baseline PPO agent training.  </li> <li>Track results in WandB and visualize in-game.  </li> </ol>"},{"location":"#description","title":"Description","text":""},{"location":"#prerequisites","title":"\ud83d\udee0\ufe0f Prerequisites","text":"<ul> <li>Git: Ensure that git is installed on your machine. Download Git</li> <li>Python 3.12: Required for the project. Download Python</li> <li>UV: Used for managing Python environments. Install UV</li> <li>Docker (optional): For DevContainer development. Download Docker</li> </ul>"},{"location":"#getting-started_1","title":"Getting started","text":"<ol> <li>Clone the repository:</li> </ol> <p><code>sh    git clone https://github.com/CogitoNTNU/DeepTactics-TrackMania.git    cd DeepTactics-TrackMania</code></p> <ol> <li>Install dependencies:</li> </ol> <p><code>sh    uv sync</code></p> <ol> <li>Set up pre commit (only for development):</li> </ol> <p><code>sh    uv run pre-commit install</code></p>"},{"location":"#usage","title":"Usage","text":"<p>To run the project, run the following command from the root directory of the project:</p>"},{"location":"#generate-documentation-site","title":"\ud83d\udcd6 Generate Documentation Site","text":"<p>To build and preview the documentation site locally:</p> <pre><code>uv run mkdocs build\nuv run mkdocs serve\n</code></pre> <p>This will build the documentation and start a local server at http://127.0.0.1:8000/ where you can browse the docs and API reference. Get the documentation according to the lastes commit on main by viewing the <code>gh-pages</code> branch on GitHub: https://cogitontnu.github.io/DeepTactics-TrackMania/.</p>"},{"location":"#testing","title":"Testing","text":"<p>To run the test suite, run the following command from the root directory of the project:</p> <pre><code>uv run pytest --doctest-modules --cov=src --cov-report=html\n</code></pre>"},{"location":"#team","title":"\ud83d\udc65 Team","text":"<p>This project would not have been possible without the hard work and dedication of all of the contributors. Thank you for the time and effort you have put into making this project a reality.</p>"},{"location":"#project-leads","title":"Project Leads","text":"<sub>Ludvig \u00d8vrevik</sub> <sub>Project Lead</sub> <sub>Brage Kvamme</sub> <sub>Project Lead</sub> <sub>Edvard Klavenes</sub> <sub>Project Member</sub> <sub>Simen F\u00f8rdestr\u00f8m Verhoeven</sub> <sub>Project Member</sub>"},{"location":"#license","title":"License","text":"<p>Distributed under the MIT License. See <code>LICENSE</code> for more information.</p>"},{"location":"reference/src/DQN/","title":"DQN","text":""},{"location":"reference/src/DQN/#src.DQN","title":"src.DQN","text":""},{"location":"reference/src/IQN/","title":"IQN","text":""},{"location":"reference/src/IQN/#src.IQN","title":"src.IQN","text":"<p>Classes:</p> <ul> <li> <code>IQN</code>           \u2013            </li> </ul>"},{"location":"reference/src/IQN/#src.IQN.IQN","title":"IQN","text":"<pre><code>IQN(n_tau_train=64, n_tau_action=64, cosine_dim=32, learning_rate=0.00025, batch_size=64, discount_factor=0.99, use_prioritized_replay=True, alpha=0.6, beta=0.4, beta_increment=0.001)\n</code></pre> <p>Methods:</p> <ul> <li> <code>get_best_action</code>             \u2013              <p>Get the best action for a given observation using the provided network.</p> </li> </ul> Source code in <code>src/IQN.py</code> <pre><code>def __init__(self,\n             n_tau_train=64,\n             n_tau_action=64,\n             cosine_dim=32,\n             learning_rate=0.00025,\n             batch_size=64,\n             discount_factor=0.99,\n             use_prioritized_replay=True,\n             alpha=0.6,\n             beta=0.4,\n             beta_increment=0.001,\n             ):\n    self.device = torch.device(\n        \"cuda\"\n        if torch.cuda.is_available()\n        else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n    )\n\n    # Store configuration for W&amp;B logging\n    self.config = {\n        'n_tau_train': n_tau_train,\n        'n_tau_action': n_tau_action,\n        'cosine_dim': cosine_dim,\n        'learning_rate': learning_rate,\n        'batch_size': batch_size,\n        'discount_factor': discount_factor,\n        'use_prioritized_replay': use_prioritized_replay,\n        'alpha': alpha,\n        'beta': beta,\n        'beta_increment': beta_increment,\n    }\n\n    self.n_tau_train = n_tau_train\n    self.n_tau_action = n_tau_action\n\n    self.policy_network = Network(cosine_dim=cosine_dim).to(self.device)\n    self.target_network = Network(cosine_dim=cosine_dim).to(self.device)\n\n    self.use_prioritized_replay = use_prioritized_replay\n    self.beta_increment = beta_increment\n    if use_prioritized_replay:\n        self.replay_buffer = PrioritizedReplayBuffer(alpha=alpha, beta=beta, storage=LazyTensorStorage(max_size=10000), batch_size=batch_size)\n    else:\n        self.replay_buffer = ReplayBuffer(storage=LazyTensorStorage(max_size=10000), batch_size=batch_size)\n\n    self.batch_size = batch_size\n    self.discount_factor = discount_factor\n    self.optimizer = torch.optim.AdamW(self.policy_network.parameters(), lr=learning_rate)\n</code></pre>"},{"location":"reference/src/IQN/#src.IQN.IQN.get_best_action","title":"get_best_action","text":"<pre><code>get_best_action(network: Module, obs: Tensor, n_tau=None)\n</code></pre> <p>Get the best action for a given observation using the provided network.</p> Source code in <code>src/IQN.py</code> <pre><code>def get_best_action(self, network: nn.Module, obs: torch.Tensor, n_tau=None):\n    \"\"\"Get the best action for a given observation using the provided network.\"\"\"\n    if n_tau is None:\n        n_tau = self.n_tau_train\n    with torch.no_grad():\n        action_quantiles, _ = network.forward(obs.to(self.device), n_tau)\n        q_values = action_quantiles.mean(dim=1)\n        return q_values.argmax(dim=1)\n</code></pre>"},{"location":"reference/src/","title":"src","text":""},{"location":"reference/src/#src","title":"src","text":"<p>Modules:</p> <ul> <li> <code>IQN</code>           \u2013            </li> </ul>"},{"location":"reference/src/env/","title":"env","text":""},{"location":"reference/src/env/#src.env","title":"src.env","text":""}]}